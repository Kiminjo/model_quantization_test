import os
import time
import argparse
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
import logging
import re
from datetime import datetime

def setup_logging():
    """ë¡œê¹…ì„ íŒŒì¼ê³¼ ì½˜ì†”ì— ë™ì‹œì— ì„¤ì •"""
    # ë¡œê·¸ í´ë” ìƒì„±
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ê³¼ í˜„ì¬ ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
    script_name = Path(__file__).stem  # compare_platform_speed
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = log_dir / f"{script_name}_{timestamp}.log"
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),  # íŒŒì¼ ì¶œë ¥
            logging.StreamHandler()  # ì½˜ì†” ì¶œë ¥
        ]
    )
    
    logging.info(f"ğŸ“ ë¡œê·¸ íŒŒì¼ ìƒì„±: {log_filename}")
    return str(log_filename)

# ë¡œê¹… ì„¤ì • ì‹¤í–‰
setup_logging()

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    from ultralytics import YOLO
    import torch
    from PIL import Image
except ImportError as e:
    logging.error(f"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    logging.info("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install ultralytics torch matplotlib pillow")
    exit()


class ModelSpeedBenchmarker:
    """YOLO ëª¨ë¸ ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self, data_path: str = 'coco128.yaml', device: str = 'auto'):
        self.data_path = data_path
        self.device = self._setup_device(device)
        self.results = {}
        
    def _setup_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device in ['auto', 'cuda']:
            if torch.cuda.is_available():
                logging.info(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥. GPU: {torch.cuda.get_device_name(0)}")
                return 'cuda'
            else:
                logging.warning("âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
                return 'cpu'
        return device

    def _run_inference_benchmark(self, model, dummy_input, model_name: str) -> Dict:
        """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ ê³µí†µ ë¡œì§"""
        inference_times = []
        
        # ì›Œë°ì—…
        logging.info(f"  ğŸ’¨ {model_name} ì›Œë°ì—… ì¤‘...")
        for _ in range(10):
            try:
                _ = model(dummy_input, verbose=False, device=self.device)
            except Exception as e:
                logging.warning(f"  âš ï¸  {model_name} ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜: {e}")
                break
        
        # ì‹¤ì œ ì¸¡ì •
        logging.info(f"  â±ï¸  {model_name} ì¶”ë¡  ì†ë„ ì¸¡ì • ì¤‘...")
        successful_runs = 0
        for i in range(100):
            try:
                if self.device == 'cuda': torch.cuda.synchronize()
                start_time = time.time()
                _ = model(dummy_input, verbose=False, device=self.device)
                if self.device == 'cuda': torch.cuda.synchronize()
                end_time = time.time()
                inference_times.append(end_time - start_time)
                successful_runs += 1
            except Exception as e:
                if i == 0: logging.warning(f"  âš ï¸  {model_name} ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        if successful_runs == 0:
            raise Exception(f"{model_name}ì˜ ëª¨ë“  ì¶”ë¡  ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        avg_inference_time = np.mean(inference_times) * 1000  # ms
        fps = 1.0 / np.mean(inference_times)
        
        logging.info(f"  âœ… {model_name} ì™„ë£Œ - FPS: {fps:.2f} ({successful_runs}/100 ì„±ê³µ)")
        return {'avg_inference_time_ms': float(avg_inference_time), 'fps': float(fps), 'successful_runs': successful_runs}

    def benchmark_pt(self, model_path: str) -> Dict:
        """PyTorch ëª¨ë¸(.pt) ë²¤ì¹˜ë§ˆí¬"""
        logging.info("ğŸ” PyTorch ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘...")
        try:
            model = YOLO(model_path).to(self.device)
            logging.info(f"  ğŸ’» ëª¨ë¸ì„ {model.device} ë””ë°”ì´ìŠ¤ë¡œ ì´ë™")
            dummy_input, height, width = self._create_dummy_input(model)
            logging.info(f"  ğŸ“ ê°ì§€ëœ ì…ë ¥ í¬ê¸°: {height}x{width}")
            
            speed_results = self._run_inference_benchmark(model, dummy_input, "PyTorch")
            return {**{'model_type': 'PyTorch'}, **speed_results}
        except Exception as e:
            logging.error(f"  âŒ PyTorch ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return self._empty_results('PyTorch')

    def benchmark_onnx(self, model_path: str) -> Dict:
        """ONNX ëª¨ë¸(.onnx) ë²¤ì¹˜ë§ˆí¬"""
        logging.info("ğŸ” ONNX ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘...")
        try:
            model = YOLO(model_path, task='detect')
            logging.info(f"  ğŸ’» ëª¨ë¸ ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {self.device}")
            dummy_input, height, width = self._create_dummy_input(model)
            logging.info(f"  ğŸ“ ê°ì§€ëœ ì…ë ¥ í¬ê¸°: {height}x{width}")
            
            speed_results = self._run_inference_benchmark(model, dummy_input, "ONNX")
            return {**{'model_type': 'ONNX'}, **speed_results}
        except Exception as e:
            logging.error(f"  âŒ ONNX ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return self._empty_results('ONNX')

    def benchmark_tensorrt(self, model_path: str) -> Dict:
        """TensorRT ëª¨ë¸(.engine) ë²¤ì¹˜ë§ˆí¬"""
        logging.info("ğŸ” TensorRT ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘...")
        try:
            model = YOLO(model_path, task='detect')
            logging.info(f"  ğŸ’» ëª¨ë¸ ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {self.device}")
            
            # TensorRT ëª¨ë¸ ìƒì„¸ ì •ë³´ ë¡œê¹… (C++ ì¶”ë¡ ê¸° ê°œë°œìš©)
            model_info = self._extract_tensorrt_model_info(model, model_path)
            self._log_tensorrt_model_details(model_info, "TensorRT")
            
            dummy_input, height, width = self._create_dummy_input(model)
            logging.info(f"  ğŸ“ ê°ì§€ëœ ì…ë ¥ í¬ê¸°: {height}x{width}")

            speed_results = self._run_inference_benchmark(model, dummy_input, "TensorRT")
            return {
                **{'model_type': 'TensorRT'}, 
                **speed_results,
                **model_info  # TensorRT ìƒì„¸ ì •ë³´ ì¶”ê°€
            }
        except Exception as e:
            logging.error(f"  âŒ TensorRT ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return self._empty_results('TensorRT')
    
    def _extract_tensorrt_model_info(self, model, model_path: str) -> Dict:
        """TensorRT ëª¨ë¸ì˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ (C++ ì¶”ë¡ ê¸° ê°œë°œìš©)"""
        model_info = {
            'engine_path': model_path,
            'input_tensors': [],
            'output_tensors': [],
            'num_classes': 80,  # COCO ê¸°ë³¸ê°’
            'class_names': [],
            'engine_file_size_mb': 0.0,
            'tensorrt_version': 'unknown',
            'precision': 'unknown',
            'max_batch_size': 1,
            'workspace_size': 'unknown'
        }
        
        try:
            # ì—”ì§„ íŒŒì¼ í¬ê¸°
            if os.path.exists(model_path):
                file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB
                model_info['engine_file_size_mb'] = round(file_size, 2)
            
            # Ultralytics ëª¨ë¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
            if hasattr(model, 'model') and hasattr(model.model, 'model'):
                try:
                    # ì…ë ¥/ì¶œë ¥ ì •ë³´ ì¶”ì¶œ ì‹œë„
                    dummy_input, height, width = self._create_dummy_input(model)
                    results = model(dummy_input, verbose=False)
                    
                    # ì…ë ¥ í…ì„œ ì •ë³´
                    model_info['input_tensors'] = [{
                        'name': 'images',
                        'shape': [1, 3, height, width],
                        'dtype': 'float32',
                        'format': 'NCHW'
                    }]
                    
                    # ì¶œë ¥ í…ì„œ ì •ë³´ (resultsë¡œë¶€í„° ì¶”ì¶œ)
                    if hasattr(results[0], 'boxes') and results[0].boxes is not None:
                        boxes = results[0].boxes
                        if hasattr(boxes, 'data') and boxes.data is not None:
                            box_shape = list(boxes.data.shape) if hasattr(boxes.data, 'shape') else [0, 6]
                            
                            # ìƒ˜í”Œ ì¶œë ¥ê°’ ë¡œê¹… (ì²˜ìŒ ëª‡ ê°œë§Œ)
                            sample_data = boxes.data[:3] if len(boxes.data) > 0 else None
                            
                            model_info['output_tensors'].append({
                                'name': 'detections',
                                'shape': box_shape,
                                'dtype': 'float32',
                                'description': 'Detection boxes with confidence and class (x1,y1,x2,y2,conf,class)',
                                'sample_values': sample_data.tolist() if sample_data is not None else None
                            })
                    
                    # í´ë˜ìŠ¤ ì •ë³´
                    if hasattr(model, 'names') and model.names:
                        model_info['class_names'] = list(model.names.values())
                        model_info['num_classes'] = len(model.names)
                
                except Exception as e:
                    logging.warning(f"  âš ï¸  ëª¨ë¸ ì¶”ë¡  ì¤‘ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # íŒŒì¼ëª…ì—ì„œ precision ì¶”ì •
            model_path_lower = model_path.lower()
            if 'fp32' in model_path_lower:
                model_info['precision'] = 'FP32'
            elif 'fp16' in model_path_lower:
                model_info['precision'] = 'FP16'
            elif 'int8' in model_path_lower:
                model_info['precision'] = 'INT8'
            elif any(x in model_path_lower for x in ['ptq', 'qat']):
                model_info['precision'] = 'INT8'
            
        except Exception as e:
            logging.warning(f"  âš ï¸  TensorRT ëª¨ë¸ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return model_info
    
    def _log_tensorrt_model_details(self, model_info: Dict, model_name: str):
        """TensorRT ëª¨ë¸ ìƒì„¸ ì •ë³´ ë¡œê¹… (C++ ì¶”ë¡ ê¸° ê°œë°œìš©)"""
        logging.info(f"\n{'='*60}")
        logging.info(f"ğŸ”§ TensorRT ëª¨ë¸ ìƒì„¸ ì •ë³´ - {model_name}")
        logging.info(f"{'='*60}")
        
        # ê¸°ë³¸ ì •ë³´
        logging.info(f"ğŸ“ Engine íŒŒì¼: {model_info['engine_path']}")
        logging.info(f"ğŸ“ íŒŒì¼ í¬ê¸°: {model_info['engine_file_size_mb']} MB")
        logging.info(f"ğŸ¯ ì •ë°€ë„: {model_info['precision']}")
        logging.info(f"ğŸ“¦ ìµœëŒ€ ë°°ì¹˜ í¬ê¸°: {model_info['max_batch_size']}")
        
        # ì…ë ¥ í…ì„œ ì •ë³´
        logging.info(f"\nğŸ” ì…ë ¥ í…ì„œ ì •ë³´:")
        for i, tensor in enumerate(model_info['input_tensors']):
            logging.info(f"  ì…ë ¥ {i+1}:")
            logging.info(f"    - ì´ë¦„: {tensor['name']}")
            logging.info(f"    - í˜•íƒœ: {tensor['shape']}")
            logging.info(f"    - ë°ì´í„° íƒ€ì…: {tensor['dtype']}")
            logging.info(f"    - í¬ë§·: {tensor['format']}")
        
        # ì¶œë ¥ í…ì„œ ì •ë³´
        logging.info(f"\nğŸ“¤ ì¶œë ¥ í…ì„œ ì •ë³´:")
        for i, tensor in enumerate(model_info['output_tensors']):
            logging.info(f"  ì¶œë ¥ {i+1}:")
            logging.info(f"    - ì´ë¦„: {tensor['name']}")
            logging.info(f"    - í˜•íƒœ: {tensor['shape']}")
            logging.info(f"    - ë°ì´í„° íƒ€ì…: {tensor['dtype']}")
            if 'description' in tensor:
                logging.info(f"    - ì„¤ëª…: {tensor['description']}")
            if 'sample_values' in tensor and tensor['sample_values']:
                logging.info(f"    - ìƒ˜í”Œ ì¶œë ¥ê°’ (ì²˜ìŒ 3ê°œ):")
                for j, sample in enumerate(tensor['sample_values'][:3]):
                    if isinstance(sample, list) and len(sample) >= 6:
                        logging.info(f"      ê²€ì¶œ {j+1}: x1={sample[0]:.2f}, y1={sample[1]:.2f}, "
                                   f"x2={sample[2]:.2f}, y2={sample[3]:.2f}, conf={sample[4]:.3f}, class={int(sample[5])}")
        
        # í´ë˜ìŠ¤ ì •ë³´
        logging.info(f"\nğŸ·ï¸  í´ë˜ìŠ¤ ì •ë³´:")
        logging.info(f"  - í´ë˜ìŠ¤ ìˆ˜: {model_info['num_classes']}")
        if model_info['class_names'] and len(model_info['class_names']) <= 10:
            logging.info(f"  - í´ë˜ìŠ¤ ì´ë¦„: {model_info['class_names']}")
        elif model_info['class_names']:
            logging.info(f"  - í´ë˜ìŠ¤ ì´ë¦„ (ì²« 10ê°œ): {model_info['class_names'][:10]}...")
        
        # C++ êµ¬í˜„ ê°€ì´ë“œ
        logging.info(f"\nğŸ’¡ C++ TensorRT ì¶”ë¡ ê¸° êµ¬í˜„ ê°€ì´ë“œ:")
        logging.info(f"  1. Engine ë¡œë“œ: deserializeCudaEngine('{model_info['engine_path']}')")
        if model_info['input_tensors']:
            tensor = model_info['input_tensors'][0]
            logging.info(f"  2. ì…ë ¥ ë°”ì¸ë”©: bindingIsInput=true, shape={tensor['shape']}")
        if model_info['output_tensors']:
            tensor = model_info['output_tensors'][0]  
            logging.info(f"  3. ì¶œë ¥ ë°”ì¸ë”©: bindingIsInput=false, shape={tensor['shape']}")
        logging.info(f"  4. ì „ì²˜ë¦¬: BGR->RGB, Normalize [0,1], HWC->CHW")
        logging.info(f"  5. í›„ì²˜ë¦¬: NMS, ì¢Œí‘œ ë³€í™˜, ì‹ ë¢°ë„ í•„í„°ë§")
        logging.info(f"  6. ì„ê³„ê°’: confidence > 0.25, NMS IoU > 0.45")
        logging.info(f"  7. ì¢Œí‘œ í˜•ì‹: ì ˆëŒ€ ì¢Œí‘œ (í”½ì…€ ë‹¨ìœ„)")
        
        # ì¶”ê°€ êµ¬í˜„ ì°¸ê³ ì‚¬í•­
        logging.info(f"\nğŸ“‹ êµ¬í˜„ ì°¸ê³ ì‚¬í•­:")
        logging.info(f"  - TensorRT API: IRuntime, ICudaEngine, IExecutionContext")
        logging.info(f"  - ë©”ëª¨ë¦¬ ê´€ë¦¬: cudaMalloc, cudaMemcpy")
        logging.info(f"  - ë™ê¸°í™”: cudaDeviceSynchronize() ë˜ëŠ” CUDA Stream ì‚¬ìš©")
        logging.info(f"  - ë°°ì¹˜ ì²˜ë¦¬: ë™ì  ë°°ì¹˜ í¬ê¸° ì§€ì› í™•ì¸ í•„ìš”")
        
        logging.info(f"{'='*60}\n")

    def _get_model_input_size(self, model) -> Tuple[int, int]:
        """ëª¨ë¸ì˜ ì…ë ¥ í¬ê¸°ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€"""
        try:
            # Ultralytics ëª¨ë¸ì—ì„œ imgsz ì†ì„± í™•ì¸
            if hasattr(model, 'overrides') and 'imgsz' in model.overrides:
                imgsz = model.overrides['imgsz']
                if isinstance(imgsz, int): return imgsz, imgsz
                if isinstance(imgsz, (list, tuple)) and len(imgsz) == 2: return imgsz[0], imgsz[1]
            
            # ëª¨ë¸ argsì—ì„œ imgsz í™•ì¸
            if hasattr(model, 'args') and hasattr(model.args, 'imgsz'):
                imgsz = model.args.imgsz
                if isinstance(imgsz, int): return imgsz, imgsz
                if isinstance(imgsz, (list, tuple)) and len(imgsz) >= 2: return imgsz[0], imgsz[1]
            
            # ì¼ë°˜ì ì¸ YOLO ì…ë ¥ í¬ê¸°ë“¤ì„ ì‹œë„í•´ë´„ (ì‘ì€ í¬ê¸°ë¶€í„°)
            # common_sizes = [320, 384, 400, 416, 480, 512, 640]
            common_sizes = [416]
            
            for size in common_sizes:
                try:
                    # PIL Imageë¡œ í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
                    from PIL import Image
                    import numpy as np
                    test_img = Image.fromarray(np.random.randint(0, 255, (size, size, 3), dtype=np.uint8))
                    _ = model(test_img, verbose=False)
                    logging.info(f"  ğŸ“ ê°ì§€ëœ ëª¨ë¸ ì…ë ¥ í¬ê¸°: {size}x{size}")
                    return size, size  # ì„±ê³µí•˜ë©´ í•´ë‹¹ í¬ê¸° ë°˜í™˜
                except Exception as e:
                    error_msg = str(e).lower()
                    if "input" in error_msg and "size" in error_msg:
                        # ì˜¤ë¥˜ ë©”ì‹œì§€ì—ì„œ ëª¨ë¸ì˜ ìµœëŒ€ í¬ê¸° ì¶”ì¶œ ì‹œë„
                        # ì˜ˆ: "max model size (1, 3, 416, 416)"
                        import re
                        match = re.search(r'max model size.*?(\d+),\s*(\d+)\)', error_msg)
                        if match:
                            max_height, max_width = int(match.group(1)), int(match.group(2))
                            logging.info(f"  ğŸ“ ì˜¤ë¥˜ ë©”ì‹œì§€ì—ì„œ ì¶”ì¶œí•œ ëª¨ë¸ í¬ê¸°: {max_height}x{max_width}")
                            return max_height, max_width
                        continue
                    # ë‹¤ë¥¸ íƒ€ì…ì˜ ì—ëŸ¬ëŠ” í•´ë‹¹ í¬ê¸°ê°€ ë§ë‹¤ê³  ê°€ì •
                    return size, size
            
            # ëª¨ë“  í¬ê¸°ê°€ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ê°’ ë°˜í™˜ (ê°€ì¥ ì‘ì€ í¬ê¸°)
            logging.warning("  âš ï¸  ëª¨ë¸ ì…ë ¥ í¬ê¸° ìë™ ê°ì§€ ì‹¤íŒ¨. ê¸°ë³¸ê°’ 416x416 ì‚¬ìš©")
            return 416, 416
            
        except Exception:
            # ê°ì§€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            logging.warning("  âš ï¸  ëª¨ë¸ ì…ë ¥ í¬ê¸° ê°ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ê¸°ë³¸ê°’ 416x416 ì‚¬ìš©")
            return 416, 416
    
    def _create_dummy_input(self, model) -> Tuple[Image.Image, int, int]:
        """ëª¨ë¸ì— ë§ëŠ” ë”ë¯¸ ì…ë ¥ ìƒì„±"""
        height, width = self._get_model_input_size(model)
        dummy_img = Image.fromarray(np.random.randint(0, 255, (height, width, 3), dtype=np.uint8), mode='RGB')
        return dummy_img, height, width
    
    def _empty_results(self, model_type: str) -> Dict:
        """ë¹ˆ ê²°ê³¼ ë°˜í™˜"""
        return {'model_type': model_type, 'avg_inference_time_ms': 0.0, 'fps': 0.0, 'successful_runs': 0}
    
    def compare_models(self, model_configs: List[Dict]) -> Dict[str, Dict]:
        """ì—¬ëŸ¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        results = {}
        for config in model_configs:
            model_path = config['path']
            model_name = config['name']
            model_type = config['type'].lower()
            
            if not os.path.exists(model_path):
                logging.warning(f"âš ï¸  ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
                results[model_name] = self._empty_results(model_type.upper())
                continue
            
            if model_type == 'pt':
                results[model_name] = self.benchmark_pt(model_path)
            elif model_type == 'onnx':
                results[model_name] = self.benchmark_onnx(model_path)
            elif model_type in ['tensorrt', 'trt', 'engine']:
                results[model_name] = self.benchmark_tensorrt(model_path)
            else:
                logging.warning(f"âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
                results[model_name] = self._empty_results('UNKNOWN')
        
        self.results = results
        return results

    def plot_speed_comparison(self, save_path: str = 'results/speed_comparison.png', show_plot: bool = True):
        """ì¶”ë¡  ì†ë„ ë¹„êµ ê²°ê³¼ ì‹œê°í™”"""
        if not self.results:
            logging.warning("âŒ ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        model_names = list(self.results.keys())
        inference_times = [self.results[name]['avg_inference_time_ms'] for name in model_names]
        fps_values = [self.results[name]['fps'] for name in model_names]

        fig, axes = plt.subplots(2, 1, figsize=(10, 10))
        fig.suptitle('Model Inference Speed Comparison (INT8 Standard)', fontsize=16, fontweight='bold')
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']

        # ì„œë¸Œí”Œë¡¯ 1: ì¶”ë¡  ì‹œê°„
        ax1 = axes[0]
        bars1 = ax1.bar(model_names, inference_times, color=colors, alpha=0.8, edgecolor='black')
        ax1.set_title('Average Inference Time (Lower is Better)', fontweight='bold')
        ax1.set_ylabel('Time (ms)', fontweight='bold')
        ax1.grid(True, alpha=0.3)
        for bar in bars1:
            yval = bar.get_height()
            if yval > 0:
                ax1.text(bar.get_x() + bar.get_width()/2.0, yval + max(inference_times)*0.01, f'{yval:.2f}ms', ha='center', va='bottom', fontweight='bold')

        # ì„œë¸Œí”Œë¡¯ 2: FPS
        ax2 = axes[1]
        bars2 = ax2.bar(model_names, fps_values, color=colors, alpha=0.8, edgecolor='black')
        ax2.set_title('Frames Per Second (Higher is Better)', fontweight='bold')
        ax2.set_ylabel('FPS', fontweight='bold')
        ax2.grid(True, alpha=0.3)
        for bar in bars2:
            yval = bar.get_height()
            if yval > 0:
                ax2.text(bar.get_x() + bar.get_width()/2.0, yval + max(fps_values)*0.01, f'{yval:.1f}', ha='center', va='bottom', fontweight='bold')

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        
        # ì €ì¥ ê²½ë¡œì˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logging.info(f"ğŸ“Š ê²°ê³¼ ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")

        if show_plot:
            plt.show()

    def print_summary_table(self):
        """ê²°ê³¼ë¥¼ í…Œì´ë¸” í˜•íƒœë¡œ ì¶œë ¥"""
        if not self.results:
            logging.warning("âŒ ì¶œë ¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        summary = "\n" + "="*70 + "\n"
        summary += "ğŸ† MODEL INFERENCE SPEED COMPARISON SUMMARY\n"
        summary += "="*70 + "\n"
        summary += f"{'Model Name':<15} {'Type':<10} {'Inf Time(ms)':<20} {'FPS':<15} {'Runs':<10}\n"
        summary += "-"*70 + "\n"

        for name, result in self.results.items():
            summary += (f"{name:<15} {result['model_type']:<10} {result['avg_inference_time_ms']:<20.2f} "
                        f"{result['fps']:<15.1f} {result['successful_runs']:<10}\n")

        summary += "-"*70 + "\n"
        valid_results = {k: v for k, v in self.results.items() if v['successful_runs'] > 0}
        if valid_results:
            best_speed = max(valid_results.items(), key=lambda x: x[1]['fps'])
            summary += f"ğŸš€ Best FPS: {best_speed[0]} ({best_speed[1]['fps']:.1f})\n"
        else:
            summary += "âš ï¸  ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
        summary += "="*70
        logging.info(summary)

    def save_results(self, save_path: str = 'results/speed_results.json'):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        # ì €ì¥ ê²½ë¡œì˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, 'w') as f:
            json.dump(self.results, f, indent=2)
        logging.info(f"ğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")


def main():
    parser = argparse.ArgumentParser(description='YOLO Model Inference Speed Comparison (INT8 Standard)')
    parser.add_argument('--device', type=str, default='auto', choices=['auto', 'cpu', 'cuda'], help='Device to use')
    parser.add_argument('--save-plot', type=str, default='results/speed_comparison.png', help='Plot save path')
    parser.add_argument('--save-results', type=str, default='results/speed_results.json', help='Results save path')
    parser.add_argument('--no-show', action='store_true', help='Do not show plot')
    
    parser.add_argument('--pt-model', type=str, required=True, help='PyTorch model path (.pt)')
    parser.add_argument('--onnx-model', type=str, required=True, help='ONNX model path (.onnx)')
    parser.add_argument('--trt-model', type=str, required=True, help='TensorRT model path (.engine)')
    
    args = parser.parse_args()
    
    model_configs = [
        {'path': args.pt_model, 'name': 'PyTorch', 'type': 'pt'},
        {'path': args.onnx_model, 'name': 'ONNX', 'type': 'onnx'},
        {'path': args.trt_model, 'name': 'TensorRT', 'type': 'tensorrt'},
    ]

    benchmarker = ModelSpeedBenchmarker(device=args.device)
    
    logging.info("ğŸš€ YOLO ëª¨ë¸ ì¶”ë¡  ì†ë„ ë¹„êµë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    logging.info(f"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {benchmarker.device}")
    
    benchmarker.compare_models(model_configs)
    benchmarker.print_summary_table()
    benchmarker.plot_speed_comparison(save_path=args.save_plot, show_plot=not args.no_show)
    benchmarker.save_results(save_path=args.save_results)
    
    logging.info("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
