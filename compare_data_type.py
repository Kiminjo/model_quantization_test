import os
import time
import argparse
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    from ultralytics import YOLO
    import torch
    from PIL import Image
except ImportError as e:
    logging.error(f"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    logging.info("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install ultralytics torch matplotlib pillow")
    exit()


class ModelPerformanceBenchmarker:
    """YOLO ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤ (mAP ë¹„êµ ì¤‘ì‹¬)"""
    
    def __init__(self, data_path: str = 'coco128.yaml', device: str = 'auto'):
        self.data_path = data_path
        self.device = self._setup_device(device)
        self.results = {}
        
    def _setup_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device in ['auto', 'cuda']:
            if torch.cuda.is_available():
                logging.info(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥. GPU: {torch.cuda.get_device_name(0)}")
                return 'cuda'
            else:
                logging.warning("âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
                return 'cpu'
        return device

    def benchmark_tensorrt(self, model_path: str, model_name: str) -> Dict:
        """Ultralyticsë¥¼ ì‚¬ìš©í•œ TensorRT ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ (ì„±ëŠ¥ ë° ì†ë„)"""
        logging.info(f"ğŸ” {model_name} ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘...")
        
        try:
            model = YOLO(model_path, task='detect')
            
            effective_device = 'cuda' if self.device == 'cuda' and torch.cuda.is_available() else 'cpu'
            logging.info(f"  ğŸ’» ëª¨ë¸ ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {effective_device}")
            
            # ëª¨ë¸ ì…ë ¥ í¬ê¸° ê°ì§€
            height, width = self._get_model_input_size(model)
            logging.info(f"  ğŸ“ ê°ì§€ëœ ì…ë ¥ í¬ê¸°: {height}x{width}")
            
            # 1. Validation ì‹¤í–‰ (mAP ê³„ì‚°)
            logging.info("  ğŸ“Š mAP ê³„ì‚° ì¤‘...")
            val_results = model.val(data=self.data_path, verbose=False, imgsz=height, device=self.device)
            map50_95 = val_results.box.map if hasattr(val_results.box, 'map') else 0.0
            map50 = val_results.box.map50 if hasattr(val_results.box, 'map50') else 0.0
            logging.info(f"  âœ… ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ - mAP50-95: {map50_95:.4f}, mAP50: {map50:.4f}")

            # 2. ì¶”ë¡  ì†ë„ ì¸¡ì •
            dummy_input, _, _ = self._create_dummy_input(model)
            speed_results = self._run_inference_benchmark(model, dummy_input, model_name)
            
            results = {
                'model_type': 'TensorRT',
                'map50_95': float(map50_95),
                'map50': float(map50),
                'input_size': f"{height}x{width}",
                **speed_results
            }
            
            return results
            
        except Exception as e:
            logging.error(f"  âŒ {model_name} ë²¤ì¹˜ë§ˆí‚¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return self._empty_results('TensorRT')
            
    def _get_model_input_size(self, model) -> Tuple[int, int]:
        """ëª¨ë¸ì˜ ì…ë ¥ í¬ê¸°ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€"""
        try:
            # Ultralytics ëª¨ë¸ì—ì„œ imgsz ì†ì„± í™•ì¸
            if hasattr(model, 'overrides') and 'imgsz' in model.overrides:
                imgsz = model.overrides['imgsz']
                if isinstance(imgsz, int): return imgsz, imgsz
                if isinstance(imgsz, (list, tuple)) and len(imgsz) == 2: return imgsz[0], imgsz[1]
            
            # ëª¨ë¸ argsì—ì„œ imgsz í™•ì¸
            if hasattr(model, 'args') and hasattr(model.args, 'imgsz'):
                imgsz = model.args.imgsz
                if isinstance(imgsz, int): return imgsz, imgsz
                if isinstance(imgsz, (list, tuple)) and len(imgsz) >= 2: return imgsz[0], imgsz[1]
            
            # ì¼ë°˜ì ì¸ YOLO ì…ë ¥ í¬ê¸°ë“¤ì„ ì‹œë„í•´ë´„ (ì‘ì€ í¬ê¸°ë¶€í„°)
            # common_sizes = [320, 384, 400, 416, 480, 512, 640]
            common_sizes = [416]
            
            for size in common_sizes:
                try:
                    # PIL Imageë¡œ í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
                    from PIL import Image
                    import numpy as np
                    test_img = Image.fromarray(np.random.randint(0, 255, (size, size, 3), dtype=np.uint8))
                    _ = model(test_img, verbose=False)
                    logging.info(f"  ğŸ“ ê°ì§€ëœ ëª¨ë¸ ì…ë ¥ í¬ê¸°: {size}x{size}")
                    return size, size  # ì„±ê³µí•˜ë©´ í•´ë‹¹ í¬ê¸° ë°˜í™˜
                except Exception as e:
                    error_msg = str(e).lower()
                    if "input" in error_msg and "size" in error_msg:
                        # ì˜¤ë¥˜ ë©”ì‹œì§€ì—ì„œ ëª¨ë¸ì˜ ìµœëŒ€ í¬ê¸° ì¶”ì¶œ ì‹œë„
                        # ì˜ˆ: "max model size (1, 3, 416, 416)"
                        import re
                        match = re.search(r'max model size.*?(\d+),\s*(\d+)\)', error_msg)
                        if match:
                            max_height, max_width = int(match.group(1)), int(match.group(2))
                            logging.info(f"  ğŸ“ ì˜¤ë¥˜ ë©”ì‹œì§€ì—ì„œ ì¶”ì¶œí•œ ëª¨ë¸ í¬ê¸°: {max_height}x{max_width}")
                            return max_height, max_width
                        continue
                    # ë‹¤ë¥¸ íƒ€ì…ì˜ ì—ëŸ¬ëŠ” í•´ë‹¹ í¬ê¸°ê°€ ë§ë‹¤ê³  ê°€ì •
                    return size, size
            
            # ëª¨ë“  í¬ê¸°ê°€ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ê°’ ë°˜í™˜ (ê°€ì¥ ì‘ì€ í¬ê¸°)
            logging.warning("  âš ï¸  ëª¨ë¸ ì…ë ¥ í¬ê¸° ìë™ ê°ì§€ ì‹¤íŒ¨. ê¸°ë³¸ê°’ 416x416 ì‚¬ìš©")
            return 416, 416
            
        except Exception:
            # ê°ì§€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            logging.warning("  âš ï¸  ëª¨ë¸ ì…ë ¥ í¬ê¸° ê°ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ê¸°ë³¸ê°’ 416x416 ì‚¬ìš©")
            return 416, 416
            
    def _create_dummy_input(self, model) -> Tuple[Image.Image, int, int]:
        """ëª¨ë¸ì— ë§ëŠ” ë”ë¯¸ ì…ë ¥ ìƒì„±"""
        height, width = self._get_model_input_size(model)
        dummy_img = Image.fromarray(np.random.randint(0, 255, (height, width, 3), dtype=np.uint8), mode='RGB')
        return dummy_img, height, width

    def _run_inference_benchmark(self, model, dummy_input, model_name: str) -> Dict:
        """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ ê³µí†µ ë¡œì§"""
        inference_times = []
        
        # ì›Œë°ì—…
        logging.info(f"  ğŸ’¨ {model_name} ì›Œë°ì—… ì¤‘...")
        for _ in range(10):
            try:
                _ = model(dummy_input, verbose=False, device=self.device)
            except Exception as e:
                logging.warning(f"  âš ï¸  {model_name} ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜: {e}")
                break
        
        # ì‹¤ì œ ì¸¡ì •
        logging.info(f"  â±ï¸  {model_name} ì¶”ë¡  ì†ë„ ì¸¡ì • ì¤‘...")
        successful_runs = 0
        for i in range(100):
            try:
                if self.device == 'cuda': torch.cuda.synchronize()
                start_time = time.time()
                _ = model(dummy_input, verbose=False, device=self.device)
                if self.device == 'cuda': torch.cuda.synchronize()
                end_time = time.time()
                inference_times.append(end_time - start_time)
                successful_runs += 1
            except Exception as e:
                if i == 0: logging.warning(f"  âš ï¸  {model_name} ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        if successful_runs == 0:
            logging.error(f"{model_name}ì˜ ëª¨ë“  ì¶”ë¡  ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return {'avg_inference_time_ms': 0.0, 'fps': 0.0, 'successful_runs': 0}
        
        avg_inference_time = np.mean(inference_times) * 1000  # ms
        fps = 1.0 / np.mean(inference_times)
        
        logging.info(f"  âœ… {model_name} ì†ë„ ì¸¡ì • ì™„ë£Œ - FPS: {fps:.2f} ({successful_runs}/100 ì„±ê³µ)")
        return {'avg_inference_time_ms': float(avg_inference_time), 'fps': float(fps), 'successful_runs': successful_runs}

    def _empty_results(self, model_type: str) -> Dict:
        """ë¹ˆ ê²°ê³¼ ë°˜í™˜"""
        return {
            'model_type': model_type,
            'map50_95': 0.0,
            'map50': 0.0,
            'input_size': "0x0",
            'avg_inference_time_ms': 0.0,
            'fps': 0.0,
            'successful_runs': 0,
        }
    
    def compare_models(self, model_configs: List[Dict]) -> Dict[str, Dict]:
        """ì—¬ëŸ¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        results = {}
        for i, config in enumerate(model_configs):
            model_path = config['path']
            model_name = config['name']
            
            if not os.path.exists(model_path):
                logging.warning(f"âš ï¸  ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
                results[model_name] = self._empty_results('TensorRT')
                continue
            
            results[model_name] = self.benchmark_tensorrt(model_path, model_name)

            # ë§ˆì§€ë§‰ ëª¨ë¸ì´ ì•„ë‹ˆë©´ ì ì‹œ ëŒ€ê¸°í•˜ì—¬ GPU ì•ˆì •í™”
            if i < len(model_configs) - 1:
                wait_time = 10
                logging.info(f"âœ¨ ë‹¤ìŒ ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ì „ì— {wait_time}ì´ˆ ë™ì•ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
                time.sleep(wait_time)
        
        self.results = results
        return results
    
    def plot_performance_comparison(self, save_path: str = 'results/performance_comparison_by_datatype.png', show_plot: bool = True):
        """ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì‹œê°í™”"""
        if not self.results:
            logging.warning("âŒ ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        model_names = list(self.results.keys())
        map50_95_values = [self.results[name]['map50_95'] for name in model_names]
        map50_values = [self.results[name]['map50'] for name in model_names]
        
        x = np.arange(len(model_names))
        width = 0.35
        
        fig, ax = plt.subplots(figsize=(12, 8))
        rects1 = ax.bar(x - width/2, map50_95_values, width, label='mAP50-95', color='#4ECDC4', alpha=0.8, edgecolor='black')
        rects2 = ax.bar(x + width/2, map50_values, width, label='mAP50', color='#FF6B6B', alpha=0.8, edgecolor='black')
        
        ax.set_ylabel('mAP Score', fontweight='bold')
        ax.set_title('TensorRT Model Performance Comparison (by Data Type)', fontsize=16, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(model_names, rotation=45, ha="right")
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        def autolabel(rects):
            for rect in rects:
                height = rect.get_height()
                if height > 0:
                    ax.annotate(f'{height:.4f}',
                                xy=(rect.get_x() + rect.get_width() / 2, height),
                                xytext=(0, 3),
                                textcoords="offset points",
                                ha='center', va='bottom', fontweight='bold')

        autolabel(rects1)
        autolabel(rects2)
        
        fig.tight_layout()
        
        # ì €ì¥ ê²½ë¡œì˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logging.info(f"ğŸ“Š ê²°ê³¼ ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")
        
        if show_plot:
            plt.show()

    def plot_speed_comparison(self, save_path: str = 'results/speed_comparison_by_datatype.png', show_plot: bool = True):
        """ì¶”ë¡  ì†ë„ ë¹„êµ ê²°ê³¼ ì‹œê°í™”"""
        if not self.results:
            logging.warning("âŒ ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        model_names = list(self.results.keys())
        inference_times = [self.results[name]['avg_inference_time_ms'] for name in model_names]
        fps_values = [self.results[name]['fps'] for name in model_names]

        fig, axes = plt.subplots(2, 1, figsize=(10, 10))
        fig.suptitle('TensorRT Model Speed Comparison (by Data Type)', fontsize=16, fontweight='bold')
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']

        # ì„œë¸Œí”Œë¡¯ 1: ì¶”ë¡  ì‹œê°„
        ax1 = axes[0]
        bars1 = ax1.bar(model_names, inference_times, color=colors, alpha=0.8, edgecolor='black')
        ax1.set_title('Average Inference Time (Lower is Better)', fontweight='bold')
        ax1.set_ylabel('Time (ms)', fontweight='bold')
        ax1.grid(True, alpha=0.3)
        for bar in bars1:
            yval = bar.get_height()
            if yval > 0:
                ax1.text(bar.get_x() + bar.get_width()/2.0, yval + max(inference_times)*0.01, f'{yval:.2f}ms', ha='center', va='bottom', fontweight='bold')

        # ì„œë¸Œí”Œë¡¯ 2: FPS
        ax2 = axes[1]
        bars2 = ax2.bar(model_names, fps_values, color=colors, alpha=0.8, edgecolor='black')
        ax2.set_title('Frames Per Second (Higher is Better)', fontweight='bold')
        ax2.set_ylabel('FPS', fontweight='bold')
        ax2.grid(True, alpha=0.3)
        for bar in bars2:
            yval = bar.get_height()
            if yval > 0:
                ax2.text(bar.get_x() + bar.get_width()/2.0, yval + max(fps_values)*0.01, f'{yval:.1f}', ha='center', va='bottom', fontweight='bold')

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        
        # ì €ì¥ ê²½ë¡œì˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logging.info(f"ğŸ“Š ì†ë„ ë¹„êµ ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")

        if show_plot:
            plt.show()

    def print_summary_table(self):
        """ê²°ê³¼ë¥¼ í…Œì´ë¸” í˜•íƒœë¡œ ì¶œë ¥"""
        if not self.results:
            logging.warning("âŒ ì¶œë ¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        summary = "\n" + "="*85 + "\n"
        summary += "ğŸ† MODEL PERFORMANCE & SPEED COMPARISON SUMMARY\n"
        summary += "="*85 + "\n"
        summary += f"{'Model Name':<15} {'mAP50-95':<15} {'mAP50':<15} {'Inf Time(ms)':<20} {'FPS':<15}\n"
        summary += "-"*85 + "\n"
        
        for name, result in self.results.items():
            summary += (f"{name:<15} {result['map50_95']:<15.4f} {result['map50']:<15.4f} "
                        f"{result['avg_inference_time_ms']:<20.2f} {result['fps']:<15.1f}\n")
        
        summary += "-"*85 + "\n"
        valid_results = {k: v for k, v in self.results.items() if v['successful_runs'] > 0}
        if valid_results:
            best_map = max(valid_results.items(), key=lambda x: x[1]['map50_95'])
            best_speed = max(valid_results.items(), key=lambda x: x[1]['fps'])
            summary += f"ğŸ¥‡ Best mAP50-95: {best_map[0]} ({best_map[1]['map50_95']:.4f})\n"
            summary += f"ğŸš€ Best FPS:      {best_speed[0]} ({best_speed[1]['fps']:.1f})\n"
        else:
            summary += "âš ï¸  ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
        summary += "="*85
        logging.info(summary)
        
    def save_results(self, save_path: str = 'results/fp32_fp16_int8_comparison.json'):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        # ì €ì¥ ê²½ë¡œì˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, 'w') as f:
            json.dump(self.results, f, indent=2)
        logging.info(f"ğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")


def main():
    parser = argparse.ArgumentParser(description='YOLO TensorRT Model Performance and Speed Comparison by Data Type')
    parser.add_argument('--data', type=str, default='coco128.yaml', help='Dataset config path')
    parser.add_argument('--device', type=str, default='auto', choices=['auto', 'cpu', 'cuda'], help='Device to use')
    parser.add_argument('--save-perf-plot', type=str, default='results/performance_comparison_by_datatype.png', help='Performance plot save path')
    parser.add_argument('--save-speed-plot', type=str, default='results/speed_comparison_by_datatype.png', help='Speed plot save path')
    parser.add_argument('--save-results', type=str, default='results/fp32_fp16_int8_comparison.json', help='Results JSON save path')
    parser.add_argument('--no-show', action='store_true', help='Do not show plot')
    
    parser.add_argument('--fp32-engine', type=str, help='FP32 TensorRT model path (.engine)')
    parser.add_argument('--fp16-engine', type=str, help='FP16 TensorRT model path (.engine)')
    parser.add_argument('--ptq-engine', type=str, help='INT8 PTQ TensorRT model path (.engine)')
    parser.add_argument('--qat-engine', type=str, help='INT8 QAT TensorRT model path (.engine)')
    
    args = parser.parse_args()
    
    model_configs = []
    if args.fp32_engine:
        model_configs.append({'path': args.fp32_engine, 'name': 'FP32'})
    if args.fp16_engine:
        model_configs.append({'path': args.fp16_engine, 'name': 'FP16'})
    if args.ptq_engine:
        model_configs.append({'path': args.ptq_engine, 'name': 'INT8_PTQ'})
    if args.qat_engine:
        model_configs.append({'path': args.qat_engine, 'name': 'INT8_QAT'})
    
    if not model_configs:
        logging.error("âŒ ë¹„êµí•  ëª¨ë¸ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. --fp32-engine, --fp16-engine, --ptq-engine, --qat-engine ì¤‘ í•˜ë‚˜ ì´ìƒì„ ì§€ì •í•´ì£¼ì„¸ìš”.")
        return

    benchmarker = ModelPerformanceBenchmarker(data_path=args.data, device=args.device)
    
    logging.info("ğŸš€ YOLO ëª¨ë¸ ì„±ëŠ¥ ë¹„êµë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    logging.info(f"ğŸ“Š ë°ì´í„°ì…‹: {args.data}")
    logging.info(f"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {benchmarker.device}")
    
    benchmarker.compare_models(model_configs)
    benchmarker.print_summary_table()
    benchmarker.plot_performance_comparison(save_path=args.save_perf_plot, show_plot=not args.no_show)
    benchmarker.plot_speed_comparison(save_path=args.save_speed_plot, show_plot=not args.no_show)
    benchmarker.save_results(save_path=args.save_results)
    
    logging.info("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
