# -*- coding: utf-8 -*-
"""
ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ê¸°ì¡´ ëª¨ë¸(Baseline)ê³¼ ìµœì í™”ëœ ì‹ ê·œ ëª¨ë¸(Optimized)ì˜ ì„±ëŠ¥ ë° ì†ë„ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
ì£¼ìš” ëª©ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
1. ONNX FP32 ëª¨ë¸ì„ ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
2. TensorRT INT8 QAT ëª¨ë¸ì„ ìµœì í™”ëœ ëª¨ë¸ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
3. ë‘ ëª¨ë¸ì˜ mAP(ì„±ëŠ¥)ì™€ FPS(ì†ë„)ë¥¼ ê°ê° ì¸¡ì •í•©ë‹ˆë‹¤.
4. ì„±ëŠ¥ ë° ì†ë„ ê°œì„ ìœ¨ì„ ê³„ì‚°í•˜ê³  ê²°ê³¼ë¥¼ ìš”ì•½í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.
"""

import os
import time
import argparse
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    from ultralytics import YOLO
    import torch
    from PIL import Image
except ImportError as e:
    logging.error(f"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    logging.info("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install ultralytics torch matplotlib pillow")
    exit()


class ModelComparator:
    """YOLO ëª¨ë¸ ì„±ëŠ¥ ë° ì†ë„ ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self, data_path: str = 'coco128.yaml', device: str = 'auto'):
        self.data_path = data_path
        self.device = self._setup_device(device)
        self.results = {}
        
    def _setup_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device in ['auto', 'cuda']:
            if torch.cuda.is_available():
                logging.info(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥. GPU: {torch.cuda.get_device_name(0)}")
                return 'cuda'
            else:
                logging.warning("âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
                return 'cpu'
        if self.device != 'cuda':
            logging.warning("TensorRT ëª¨ë¸ì€ CUDAì—ì„œë§Œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤. `device`ë¥¼ 'cuda'ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            return 'cuda'
        return device
    
    def benchmark_model(self, model_path: str, model_name: str, model_type: str) -> Optional[Dict]:
        """ë‹¨ì¼ ëª¨ë¸ì˜ ì„±ëŠ¥(mAP)ê³¼ ì†ë„(FPS)ë¥¼ ë²¤ì¹˜ë§ˆí‚¹"""
        logging.info(f"ğŸ” '{model_name}' ({model_type}) ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘...")
        
        if not os.path.exists(model_path):
            logging.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            return None

        try:
            model = YOLO(model_path, task='detect')
            
            # --- ì„±ëŠ¥ ì¸¡ì • (mAP) ---
            logging.info(f"  ğŸ“Š [{model_name}] mAP ê³„ì‚° ì¤‘...")
            height, width = self._get_model_input_size(model)
            val_results = model.val(data=self.data_path, verbose=False, imgsz=height, device=self.device)
            map50_95 = float(val_results.box.map) if hasattr(val_results.box, 'map') else 0.0
            map50 = float(val_results.box.map50) if hasattr(val_results.box, 'map50') else 0.0
            logging.info(f"  âœ… [{model_name}] mAP50-95: {map50_95:.4f}, mAP50: {map50:.4f}")

            # --- ì†ë„ ì¸¡ì • (FPS) ---
            logging.info(f"  â±ï¸  [{model_name}] ì¶”ë¡  ì†ë„ ì¸¡ì • ì¤‘...")
            dummy_input, _, _ = self._create_dummy_input(model, height, width)
            
            # ì›Œë°ì—…
            for _ in range(10):
                _ = model(dummy_input, verbose=False, device=self.device)

            # ì‹¤ì œ ì¸¡ì •
            inference_times = []
            for _ in range(100):
                torch.cuda.synchronize()
                start_time = time.time()
                _ = model(dummy_input, verbose=False, device=self.device)
                torch.cuda.synchronize()
                end_time = time.time()
                inference_times.append(end_time - start_time)
            
            avg_inference_time = np.mean(inference_times) * 1000  # ms
            fps = 1.0 / np.mean(inference_times)
            logging.info(f"  âœ… [{model_name}] í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_inference_time:.2f}ms, FPS: {fps:.2f}")

            return {
                'model_name': model_name,
                'model_type': model_type,
                'map50_95': map50_95,
                'map50': map50,
                'avg_inference_time_ms': avg_inference_time,
                'fps': fps,
            }

        except Exception as e:
            logging.error(f"âŒ '{model_name}' ë²¤ì¹˜ë§ˆí‚¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return None
            
    def _get_model_input_size(self, model) -> Tuple[int, int]:
        """ëª¨ë¸ì˜ ì…ë ¥ í¬ê¸°ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€"""
        try:
            # Ultralytics ëª¨ë¸ì—ì„œ imgsz ì†ì„± í™•ì¸
            if hasattr(model, 'overrides') and 'imgsz' in model.overrides:
                imgsz = model.overrides['imgsz']
                if isinstance(imgsz, int): return imgsz, imgsz
                if isinstance(imgsz, (list, tuple)) and len(imgsz) == 2: return imgsz[0], imgsz[1]
            
            # ëª¨ë¸ argsì—ì„œ imgsz í™•ì¸
            if hasattr(model, 'args') and hasattr(model.args, 'imgsz'):
                imgsz = model.args.imgsz
                if isinstance(imgsz, int): return imgsz, imgsz
                if isinstance(imgsz, (list, tuple)) and len(imgsz) >= 2: return imgsz[0], imgsz[1]
            
            # ì¼ë°˜ì ì¸ YOLO ì…ë ¥ í¬ê¸°ë“¤ì„ ì‹œë„í•´ë´„ (ì‘ì€ í¬ê¸°ë¶€í„°)
            # common_sizes = [320, 384, 400, 416, 480, 512, 640]
            common_sizes = [416]
            
            
            for size in common_sizes:
                try:
                    # PIL Imageë¡œ í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
                    from PIL import Image
                    import numpy as np
                    test_img = Image.fromarray(np.random.randint(0, 255, (size, size, 3), dtype=np.uint8))
                    _ = model(test_img, verbose=False)
                    logging.info(f"  ğŸ“ ê°ì§€ëœ ëª¨ë¸ ì…ë ¥ í¬ê¸°: {size}x{size}")
                    return size, size  # ì„±ê³µí•˜ë©´ í•´ë‹¹ í¬ê¸° ë°˜í™˜
                except Exception as e:
                    error_msg = str(e).lower()
                    if "input" in error_msg and "size" in error_msg:
                        # ì˜¤ë¥˜ ë©”ì‹œì§€ì—ì„œ ëª¨ë¸ì˜ ìµœëŒ€ í¬ê¸° ì¶”ì¶œ ì‹œë„
                        # ì˜ˆ: "max model size (1, 3, 416, 416)"
                        import re
                        match = re.search(r'max model size.*?(\d+),\s*(\d+)\)', error_msg)
                        if match:
                            max_height, max_width = int(match.group(1)), int(match.group(2))
                            logging.info(f"  ğŸ“ ì˜¤ë¥˜ ë©”ì‹œì§€ì—ì„œ ì¶”ì¶œí•œ ëª¨ë¸ í¬ê¸°: {max_height}x{max_width}")
                            return max_height, max_width
                        continue
                    # ë‹¤ë¥¸ íƒ€ì…ì˜ ì—ëŸ¬ëŠ” í•´ë‹¹ í¬ê¸°ê°€ ë§ë‹¤ê³  ê°€ì •
                    return size, size
            
            # ëª¨ë“  í¬ê¸°ê°€ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ê°’ ë°˜í™˜ (ê°€ì¥ ì‘ì€ í¬ê¸°)
            logging.warning("  âš ï¸  ëª¨ë¸ ì…ë ¥ í¬ê¸° ìë™ ê°ì§€ ì‹¤íŒ¨. ê¸°ë³¸ê°’ 416x416 ì‚¬ìš©")
            return 416, 416
            
        except Exception:
            # ê°ì§€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            logging.warning("  âš ï¸  ëª¨ë¸ ì…ë ¥ í¬ê¸° ê°ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ê¸°ë³¸ê°’ 416x416 ì‚¬ìš©")
            return 416, 416
    
    def _create_dummy_input(self, model, height, width) -> Tuple[Image.Image, int, int]:
        """ëª¨ë¸ì— ë§ëŠ” ë”ë¯¸ ì…ë ¥ ìƒì„±"""
        dummy_img = Image.fromarray(np.random.randint(0, 255, (height, width, 3), dtype=np.uint8), mode='RGB')
        return dummy_img, height, width
    
    def compare_and_summarize(self, baseline_path: str, optimized_path: str):
        """ë‘ ëª¨ë¸ì„ ë¹„êµí•˜ê³  ê²°ê³¼ë¥¼ ìš”ì•½"""
        baseline_results = self.benchmark_model(baseline_path, "ONNX FP32 (Baseline)", "ONNX")
        optimized_results = self.benchmark_model(optimized_path, "TensorRT INT8-QAT (Optimized)", "TensorRT")

        if not baseline_results or not optimized_results:
            logging.error("âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨ë¡œ ë¹„êµë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ê°œì„ ìœ¨ ê³„ì‚°
        map_improvement = 0
        if baseline_results['map50_95'] > 0:
            map_improvement = ((optimized_results['map50_95'] - baseline_results['map50_95']) / baseline_results['map50_95']) * 100
        
        speed_improvement = 0
        if baseline_results['fps'] > 0:
            speed_improvement = ((optimized_results['fps'] - baseline_results['fps']) / baseline_results['fps']) * 100
            
        self.results = {
            "baseline": baseline_results,
            "optimized": optimized_results,
            "improvements": {
                "map50_95_improvement_percent": map_improvement,
                "speed_improvement_percent_fps": speed_improvement
            }
        }
        
        self.print_summary()

    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        if not self.results:
            logging.warning("âŒ ì¶œë ¥í•  ìš”ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        base = self.results['baseline']
        opt = self.results['optimized']
        imp = self.results['improvements']

        summary = "\n" + "="*80 + "\n"
        summary += "ğŸ† MODEL COMPARISON SUMMARY: ONNX FP32 vs TensorRT INT8-QAT\n"
        summary += "="*80 + "\n"
        summary += f"{'Metric':<25} {'ONNX FP32 (Baseline)':<25} {'TensorRT INT8-QAT (Optimized)':<30}\n"
        summary += "-"*80 + "\n"
        summary += f"{'mAP50-95':<25} {base['map50_95']:<25.4f} {opt['map50_95']:<30.4f}\n"
        summary += f"{'mAP50':<25} {base['map50']:<25.4f} {opt['map50']:<30.4f}\n"
        summary += f"{'Inference Time (ms)':<25} {base['avg_inference_time_ms']:<25.2f} {opt['avg_inference_time_ms']:<30.2f}\n"
        summary += f"{'FPS':<25} {base['fps']:<25.2f} {opt['fps']:<30.2f}\n"
        summary += "="*80 + "\n"
        summary += "ğŸ“Š IMPROVEMENT ANALYSIS\n"
        summary += "-"*80 + "\n"
        summary += f"  - Performance (mAP50-95): {imp['map50_95_improvement_percent']:+.2f} % "
        summary += "(Positive is better, Negative is worse)\n"
        summary += f"  - Speed (FPS): {imp['speed_improvement_percent_fps']:+.2f} % "
        summary += "(Positive is faster, Negative is slower)\n"
        summary += "="*80
        logging.info(summary)

    def save_results(self, save_path: str = 'results/comparison_results.json'):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not self.results:
            logging.warning("âŒ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì €ì¥ ê²½ë¡œì˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, 'w') as f:
            json.dump(self.results, f, indent=2)
        logging.info(f"ğŸ’¾ ë¹„êµ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Compare ONNX FP32 and TensorRT INT8-QAT models.')
    parser.add_argument('--data', type=str, default='coco128.yaml', help='Dataset config path')
    parser.add_argument('--device', type=str, default='cuda', choices=['cuda'], help='Device to use (TensorRT requires CUDA)')
    parser.add_argument('--onnx-model', type=str, required=True, help='ONNX FP32 model path (baseline)')
    parser.add_argument('--qat-trt-model', type=str, required=True, help='QAT TensorRT model path (optimized)')
    parser.add_argument('--save-results', type=str, default='results/comparison_results.json', help='Results save path')
    
    args = parser.parse_args()
    
    comparator = ModelComparator(data_path=args.data, device=args.device)
    
    logging.info("ğŸš€ ëª¨ë¸ ë¹„êµë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    comparator.compare_and_summarize(args.onnx_model, args.qat_trt_model)
    comparator.save_results(save_path=args.save_results)
    logging.info("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()